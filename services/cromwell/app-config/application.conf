# This line is required. It pulls in default overrides from the embedded cromwell `application.conf` needed for proper
# performance of cromwell.
include required(classpath("application"))


languages {
  default: WDL
  WDL {
    versions {
      default: "development"
      "1.0" {
        # WDL draft-3 was our in-progress name for what became WDL 1.0
        language-factory = "languages.wdl.draft3.WdlDraft3LanguageFactory"
        config {
          strict-validation: true
          enabled: true
        }
      }
      "biscayne" {
        # WDL biscayne is our in-progress name for what will (probably) become WDL 1.1
        language-factory = "languages.wdl.biscayne.WdlBiscayneLanguageFactory"
        config {
          strict-validation: true
          enabled: true
        }
      }
    }
  }
  CWL {
    versions {
      default: "v1.0"
      "v1.0" {
        language-factory = "languages.cwl.CwlV1_0LanguageFactory"
        config {
          strict-validation: false
          enabled: true

          # Optional section to define a command returning paths to output files that
          # can only be known after the user's command has been run
          output-runtime-extractor {
            # Optional docker image on which the command should be run - Must have bash if provided
            docker-image = "stedolan/jq@sha256:a61ed0bca213081b64be94c5e1b402ea58bc549f457c2682a86704dd55231e09"
            # Single line command executed after the tool has run.
            # It should write to stdout the file paths that are referenced in the cwl.output.json (or any other file path
            # not already defined in the outputs of the tool and therefore unknown when the tool is run)
            command = "cat cwl.output.json 2>/dev/null  jq -r '..  .path? // .location? // empty'"
          }
        }
      }
    }
  }
}

webservice {
  port = 8000
  interface = 0.0.0.0
  binding-timeout = 20s
  instance.name = "reference"
}

system {
  # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
  # Defaults to false in server mode, and true in run mode.
  # abort-jobs-on-terminate = false

  # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,
  # in particular clearing up all queued database writes before letting the JVM shut down.
  # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.
  graceful-server-shutdown = true

  # If 'true' then when Cromwell starts up, it tries to restart incomplete workflows
  workflow-restart = false

  # Cromwell will cap the number of running workflows at N
  max-concurrent-workflows = 4

  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
  max-workflow-launch-count = 20

  # Workflows will be grouped by the value of the specified field in their workflow options.
  #
  # Currently, hog-safety will limit concurrent jobs within a hog group:
  # For each hog-group and backend, only (concurrent-job-limit / hog-factor) jobs will be run at any given time.
  #
  # In the future, hog-groups may also apply to other finite resources
  #
  # You can re-use an existing workflow option to allow it to also indicate hog-group,
  # or add a new field created specifically for hog-safety.
  #
  hog-safety {
    # Set this field in the workflow-options file to assign a hog group:
    workflow-option = "hogGroup"

    # Setting to '1' means that a hog group can use the full resources of the Cromwell instance if it needs to:
    hog-factor = 2

    # Time to wait before repeating token log messages - including "queue state" and "group X is a hog" style messages.
    # Leave at 0 to never log these types of message.
    token-log-interval-seconds = 0
  }

  # Number of seconds between workflow launches
  new-workflow-poll-rate = 5

  # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
  number-of-workflow-log-copy-workers = 10

  # Default number of cache read workers
  number-of-cache-read-workers = 25

  # Maximum scatter width per scatter node. Cromwell will fail the workflow if the scatter width goes beyond N
  max-scatter-width-per-scatter = 1000000

  # Total max. jobs that can be created per root workflow. If it goes beyond N, Cromwell will fail the workflow by:
  # - no longer creating new jobs
  # - let the jobs that have already been started finish, and then fail the workflow
  total-max-jobs-per-root-workflow = 1000000

  io {
    # Global Throttling - This is mostly useful for GCS and can be adjusted to match
    # the quota availble on the GCS API
    number-of-requests = 100000
    per = 100 seconds

    # Number of times an I/O operation should be attempted before giving up and failing it.
    number-of-attempts = 8

    # configures exponential backoff of io requests
    backpressure-backoff {
      # starting point
      min = 10 seconds
      # maximum waiting time between attempts
      max = 5 minutes
      # how much longer to wait between each attempt
      multiplier = 2
      # randomizes wait times to avoid large spikes. Must be between 0 and 1.
      randomization-factor = 0.9
    }

    # Amount of time after which an I/O operation will timeout if no response has been received.
    # Note that a timeout may result in a workflow failure so be careful not to set a timeout too low.
    # Unless you start experiencing timeouts under very heavy load there should be no reason to change the default values.
    timeout {
      default = 15 minutes
      # Copy can be a time consuming operation and its timeout can be set separately.
      copy = 1 hour
    }

    gcs {
      parallelism = 10
    }

    nio {
      parallellism = 10
    }
  }

  # Maximum number of input file bytes allowed in order to read each type.
  # If exceeded a FileSizeTooBig exception will be thrown.
  input-read-limits {

    lines = 1280000

    bool = 7

    int = 19

    float = 50

    string = 128000

    json = 1280000

    tsv = 1280000

    map = 1280000

    object = 1280000
  }

  # Rate at which Cromwell updates its instrumentation gauge metrics (e.g: Number of workflows running, queued, etc..)
  instrumentation-rate = 5 seconds

  job-rate-control {
    jobs = 10
    per = 1 second
  }

  workflow-heartbeats {
    heartbeat-interval: 2 minutes
    ttl: 10 minutes
    write-batch-size: 10000
    write-threshold: 10000
  }

  job-shell: "/bin/bash"
}

workflow-options {

  # Directory where to write per workflow logs
  workflow-log-dir: "/data/cromwell-workflow-logs"

  # When true, per workflow logs will be deleted after copying
  workflow-log-temporary: false

  # When a workflow type version is not provided on workflow submission, this specifies the default type version.
  workflow-type-version: "development"

  # AES-256 key to use to encrypt the values in `encrypted-fields`
  base64-encryption-key: "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="
}

call-caching {
  enabled = true
}

backend {
  default = "Local"
  concurrent-job-limit = 8
  providers {
    Local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
          concurrent-job-limit = 8
          run-in-background = true
          runtime-attributes = """
        String? docker
        String? docker_user
        String? docker_cpu
        String? docker_memory
        """
          submit = "${job_shell} ${script}"
          submit-docker = """
          # make sure there is no preexisting Docker CID file
          rm -f ${docker_cid}
          # run as in the original configuration without --rm flag (will remove later)
          docker run \
            --cidfile ${docker_cid} \
            ${"--cpus=" + docker_cpu} \
            ${"--memory=" + docker_memory } \
            -i \
            --entrypoint ${job_shell} \
            -v ${cwd}:${docker_cwd} \
            --cap-add=SYS_NICE \
            ${docker} ${docker_script}

          # get the return code (working even if the container was detached)
          rc=$(docker wait `cat ${docker_cid}`)

          # remove the container after waiting
          docker rm `cat ${docker_cid}`

          # return exit code
          exit $rc
        """

        kill-docker = "docker kill `cat ${docker_cid}`"

        # Root directory where Cromwell writes job results. This directory must be
        # visible and writeable by the Cromwell process as well as the jobs that Cromwell
        # launches.
        root: "/data/cromwell-executions"

        filesystems {
          local {
            localization: [
              "hard-link", "soft-link", "copy"
            ]

            caching {
              duplication-strategy: [
                "hard-link", "soft-link", "copy"
              ]

              # Possible values: file, path
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.
              hashing-strategy: "file"

              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
              check-sibling-md5: true
            }
          }
        }
      }
    }
  }
}

database {
  profile = "slick.jdbc.MySQLProfile$"
  db {
    url = "jdbc:mysql://mysql-db/cromwell_db?useSSL=false&rewriteBatchedStatements=true&allowPublicKeyRetrieval=true"
    user = "cromwell"
    password = "cromwell"
    driver = "com.mysql.cj.jdbc.Driver"
    allowPublicKeyRetrieval=true
    connectionTimeout = 10000
  }
}
